{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"tmp/gensim\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[ gensim version ]\n",
    "\n",
    "'''\n",
    "# Libraries\n",
    "\n",
    "import os \n",
    "TEMP_FOLDER = 'tmp/gensim'\n",
    "RES_FOLDER = 'res'\n",
    "from smart_open import open\n",
    "import json\n",
    "\n",
    "# gensim\n",
    "import logging \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) \n",
    "import tempfile \n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER)) \n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim') \n",
    "from gensim import corpora \n",
    "import gensim \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import csv \n",
    "import collections \n",
    "import random \n",
    "from pprint import pprint  # pretty-printer \n",
    "\n",
    "# tensorflow\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "# directories\n",
    "\n",
    "CONTENTS_SIZE = 7\n",
    "contents_files = [ os.path.join(RES_FOLDER, 'contents/data.'+str(i)) for i in range(CONTENTS_SIZE) ]\n",
    "metadata_file = os.path.join(RES_FOLDER, 'metadata.json')\n",
    "\n",
    "if not os.path.exists(TEMP_FOLDER) :\n",
    "    os.mkdir(TEMP_FOLDER)\n",
    "    \n",
    "documents_file = os.path.join(TEMP_FOLDER, 'documents_dict.json')\n",
    "words_file = os.path.join(TEMP_FOLDER, 'words_dict.json')\n",
    "preprocess_words_file = os.path.join(TEMP_FOLDER, 'preprocess_words_dict.json')\n",
    "\n",
    "all_words_list = os.path.join(TEMP_FOLDER, 'all_words')\n",
    "\n",
    "\n",
    "import string\n",
    "def normalize_text(text) :\n",
    "    # Remove Punctuation\n",
    "    text = text.strip(string.punctuation)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "[ Make Vaild Documents Dictionary ]\n",
    "- From metadata\n",
    "\n",
    "[ Make Word Dictionary ]\n",
    "\n",
    "'''\n",
    "\n",
    "documents = {}\n",
    "word_dict = {}\n",
    "\n",
    "\n",
    "with open(metadata_file, 'r') as f :\n",
    "    for raw in f :\n",
    "        line = json.JSONDecoder().decode(raw)\n",
    "        # line\n",
    "        '''\n",
    "        # {'magazine_id': 8982, \n",
    "            'user_id': '@bookdb', \n",
    "            'title': '사진으로 옮기기에도 아까운, 리치필드 국립공원', \n",
    "            'keyword_list': ['여행', '호주', '국립공원'], \n",
    "            'display_url': 'https://brunch.co.kr/@bookdb/782', \n",
    "            'sub_title': '세상 어디에도 없는 호주 Top 10', \n",
    "            'reg_ts': 1474944427000, \n",
    "            'article_id': 782, \n",
    "            'id': '@bookdb_782'}\n",
    "        '''\n",
    "        # document id\n",
    "        if line['id'] not in documents :\n",
    "            documents[line['id']] = line\n",
    "        \n",
    "        # keyword\n",
    "        for keyword in line['keyword_list'] :\n",
    "            if keyword not in word_dict :\n",
    "                word_dict[keyword] = len(word_dict)\n",
    "            \n",
    "        # title\n",
    "        new_title = []\n",
    "        for title in line['title'] :\n",
    "            for w in title.split(' ') :\n",
    "                w = normalize_text(w)\n",
    "                if w not in word_dict :\n",
    "                    word_dict[w] = len(word_dict)\n",
    "                \n",
    "                new_title.append(word_dict[w])\n",
    "                \n",
    "        documents[line['id']]['title'] = new_title\n",
    "        \n",
    "        # sub_title\n",
    "        new_sub_title = []\n",
    "        for subtitle in line['sub_title'] :\n",
    "            for w in subtitle.split(' ') :\n",
    "                w = normalize_text(w)\n",
    "                if w not in word_dict :\n",
    "                    word_dict[w] = len(word_dict)\n",
    "                \n",
    "                new_sub_title.append(word_dict[w])\n",
    "                \n",
    "        documents[line['id']]['sub_title'] = new_sub_title\n",
    "\n",
    "# save document file\n",
    "with open(documents_file, 'w') as f :\n",
    "    f.write(json.JSONEncoder().encode(documents))\n",
    "\n",
    "# save word dictionary file\n",
    "with open(words_file, 'w') as f :\n",
    "    f.write(json.JSONEncoder().encode(word_dict))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643104\n",
      "643104\n",
      "https://brunch.co.kr/@bookdb/782\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "[ Make word dictionary ]\n",
    "- From contents/data.0 ~ 6\n",
    "- Using metadata\n",
    "\n",
    "- for gensim project, word dict cannot be updated.\n",
    "  -> store all words previously\n",
    "  -> preprocess_word_dict = {}\n",
    "'''\n",
    "\n",
    "# load document file\n",
    "with open(documents_file, 'r') as f :\n",
    "    documents = json.JSONDecoder().decode(f.read())\n",
    "\n",
    "# load word dictionary file\n",
    "word_dict = {}\n",
    "with open(words_file, 'r') as f :\n",
    "    word_dict = json.JSONDecoder().decode(f.read())\n",
    "\n",
    "\n",
    "# 1. word dictionary & max length of sentences\n",
    "\n",
    "# for training gensim initialize\n",
    "preprocess_word_dict = {}\n",
    "\n",
    "# word_dict = {} : be made previously\n",
    "MAX_SENTENCE_LEN = 0\n",
    "for file_number in tqdm_notebook(range(CONTENTS_SIZE)) :\n",
    "    # read each file\n",
    "    with open(contents_files[file_number]) as f :\n",
    "        # read each line\n",
    "        for raw in f :\n",
    "            line = json.JSONDecoder().decode(raw)\n",
    "            \n",
    "            doc_id = line['id']\n",
    "            # if not exists in metadata -> ignore\n",
    "            if doc_id not in documents : continue\n",
    "                \n",
    "            morphs = line['morphs']\n",
    "            MAX_SENTENCE_LEN = max(MAX_SENTENCE_LEN, len(morphs))\n",
    "            \n",
    "            for morph in morphs :\n",
    "                if morph not in word_dict :\n",
    "                    word_dict[morph] = len(word_dict)\n",
    "                \n",
    "                if word_dict[morph] not in preprocess_word_dict :\n",
    "                    preprocess_word_dict[word_dict[morph]] = doc_id\n",
    "            \n",
    "# save word dictionary file\n",
    "with open(words_file, 'w') as f :\n",
    "    f.write(json.JSONEncoder().encode(word_dict))\n",
    "    \n",
    "# save preprocess word dictionary file\n",
    "with open(preprocess_words_file, 'w') as f :\n",
    "    f.write(json.JSONEncoder().encode(preprocess_word_dict))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643104, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_value_from_dict(dictionary, key) :\n",
    "    # if not included, update.\n",
    "    if key not in dictionary :\n",
    "        # start from 1 (not zero)\n",
    "        dictionary[key] = len(dictionary)+1\n",
    "    \n",
    "    return dictionary[key]\n",
    "\n",
    "metadata = pd.read_json(metadata_file, lines=True)\n",
    "\n",
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>display_url</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword_list</th>\n",
       "      <th>magazine_id</th>\n",
       "      <th>reg_ts</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>782</td>\n",
       "      <td>https://brunch.co.kr/@bookdb/782</td>\n",
       "      <td>@bookdb_782</td>\n",
       "      <td>[여행, 호주, 국립공원]</td>\n",
       "      <td>8982</td>\n",
       "      <td>1474944427000</td>\n",
       "      <td>세상 어디에도 없는 호주 Top 10</td>\n",
       "      <td>사진으로 옮기기에도 아까운, 리치필드 국립공원</td>\n",
       "      <td>@bookdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>https://brunch.co.kr/@kohwang56/81</td>\n",
       "      <td>@kohwang56_81</td>\n",
       "      <td>[목련꽃, 아지랑이, 동행]</td>\n",
       "      <td>12081</td>\n",
       "      <td>1463092749000</td>\n",
       "      <td></td>\n",
       "      <td>[시] 서러운 봄</td>\n",
       "      <td>@kohwang56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>https://brunch.co.kr/@hannahajink/4</td>\n",
       "      <td>@hannahajink_4</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1447997287000</td>\n",
       "      <td>무엇 때문에</td>\n",
       "      <td>무엇을 위해</td>\n",
       "      <td>@hannahajink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88</td>\n",
       "      <td>https://brunch.co.kr/@bryceandjuli/88</td>\n",
       "      <td>@bryceandjuli_88</td>\n",
       "      <td>[감정, 마음, 위로]</td>\n",
       "      <td>16315</td>\n",
       "      <td>1491055161000</td>\n",
       "      <td></td>\n",
       "      <td>싫다</td>\n",
       "      <td>@bryceandjuli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>https://brunch.co.kr/@mijeongpark/34</td>\n",
       "      <td>@mijeongpark_34</td>\n",
       "      <td>[유럽여행, 더블린, 아일랜드]</td>\n",
       "      <td>29363</td>\n",
       "      <td>1523292942000</td>\n",
       "      <td>#7. 내 친구의 집은 어디인가</td>\n",
       "      <td>Dubliner#7</td>\n",
       "      <td>@mijeongpark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                            display_url                id  \\\n",
       "0         782       https://brunch.co.kr/@bookdb/782       @bookdb_782   \n",
       "1          81     https://brunch.co.kr/@kohwang56/81     @kohwang56_81   \n",
       "2           4    https://brunch.co.kr/@hannahajink/4    @hannahajink_4   \n",
       "3          88  https://brunch.co.kr/@bryceandjuli/88  @bryceandjuli_88   \n",
       "4          34   https://brunch.co.kr/@mijeongpark/34   @mijeongpark_34   \n",
       "\n",
       "        keyword_list  magazine_id         reg_ts             sub_title  \\\n",
       "0     [여행, 호주, 국립공원]         8982  1474944427000  세상 어디에도 없는 호주 Top 10   \n",
       "1    [목련꽃, 아지랑이, 동행]        12081  1463092749000                         \n",
       "2                 []            0  1447997287000                무엇 때문에   \n",
       "3       [감정, 마음, 위로]        16315  1491055161000                         \n",
       "4  [유럽여행, 더블린, 아일랜드]        29363  1523292942000     #7. 내 친구의 집은 어디인가   \n",
       "\n",
       "                       title        user_id  \n",
       "0  사진으로 옮기기에도 아까운, 리치필드 국립공원        @bookdb  \n",
       "1                  [시] 서러운 봄     @kohwang56  \n",
       "2                     무엇을 위해   @hannahajink  \n",
       "3                         싫다  @bryceandjuli  \n",
       "4                 Dubliner#7   @mijeongpark  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [여행, 호주, 국립공원]\n",
       "1               [목련꽃, 아지랑이, 동행]\n",
       "2                            []\n",
       "3                  [감정, 마음, 위로]\n",
       "4             [유럽여행, 더블린, 아일랜드]\n",
       "5            [석유에너지, 베네수엘라, 경제]\n",
       "6           [입찰, 유치권, 부동산경매변호사]\n",
       "7                [사랑, 연애, rain]\n",
       "8                     [메일, 출판사]\n",
       "9               [도시애벌레, 공부, 동시]\n",
       "10                [패션, 에세이, 경제]\n",
       "11              [불꽃축제, 불꽃, 여의도]\n",
       "12               [주머니, 동시, 빵가게]\n",
       "13                 [생각, 과거, 무시]\n",
       "14                 [단상, 여행, 생각]\n",
       "15                 [시험, 아침조회시간]\n",
       "16             [감성에세이, 사랑, 첫사랑]\n",
       "17            [애견놀이터, 애견, 애견카페]\n",
       "18                           []\n",
       "19                 [엄마, 취향, 효도]\n",
       "20                [칼국수, 대화, 저항]\n",
       "21               [세계여행, 여행, 행복]\n",
       "22                [버스, 지하철, 친구]\n",
       "23              [그림자, 하이에나, 도시]\n",
       "24                   [자작소설, 창작]\n",
       "25               [팀워크, 성과, 개인기]\n",
       "26              [브랜드, 브랜딩, 마케팅]\n",
       "27             [일자리, 청년실업, 저출산]\n",
       "28               [취업, 사회생활, 결혼]\n",
       "29                           []\n",
       "                  ...          \n",
       "643074      [고관절스트레칭, 스트레칭, 건강]\n",
       "643075        [다낭, 베트남여행, 다낭여행]\n",
       "643076             [환자, 병원, 엄마]\n",
       "643077           [샤오미, 숄더백, IT]\n",
       "643078      [포틀랜드, 라이프스타일, 자영업]\n",
       "643079         [에니어그램, 글쓰기, 대화]\n",
       "643080       [공감에세이, 일러스트, 글쓰기]\n",
       "643081           [자유주의, 정치, 경제]\n",
       "643082       [워킹홀리데이, 아일랜드, 시작]\n",
       "643083            [분기점, 노인, 선택]\n",
       "643084             [엄마, 아내, 이름]\n",
       "643085             [남편, 아내, 중년]\n",
       "643086    [심리상담서점, 심리상담, 리지블루스]\n",
       "643087          [공포영화, 영화, 할로윈]\n",
       "643088           [스타트업, 생각, 공간]\n",
       "643089           [다이어트, 돼지, 출판]\n",
       "643090       [오디오, 라이프스타일, 스피커]\n",
       "643091             [와인, 보르도, 술]\n",
       "643092                       []\n",
       "643093      [엘리엇, 금융계열사, 현대차그룹]\n",
       "643094            [가을, 일상, 손글씨]\n",
       "643095            [서양, 유니폼, 요리]\n",
       "643096          [시계탑, 파추카, 멕시코]\n",
       "643097            [에세이, 행복, 일기]\n",
       "643098            [식물, 화분, 책임감]\n",
       "643099            [3D, UI, 제스처]\n",
       "643100        [독서모임, 경험수집, 글쓰기]\n",
       "643101           [생각, 에세이, 괴로움]\n",
       "643102          [여행, 유럽여행, 리스본]\n",
       "643103            [리스본, 여행, 유럽]\n",
       "Name: keyword_list, Length: 643104, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['keyword_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
