{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "###################\n",
    "# [ Sample Code ] #\n",
    "###################\n",
    "'''\n",
    "\n",
    "# matrix\n",
    "embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data_ph)\n",
    "\n",
    "# print\n",
    "print('Training Accuracy: {}'.format(accuracy))\n",
    "#--> Training Accuracy: 0.878171\n",
    "\n",
    "\n",
    "# normalization\n",
    "data = tf.nn.batch_norm_with_global_normalization(...)\n",
    "\n",
    "# training set, test set, cross-validation set\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "iterations = 1000\n",
    "\n",
    "# parameters - default: float32\n",
    "a_var = tf.constant(42)\n",
    "x_input = tf.placeholder(tf.float32, [None, input_size])\n",
    "y_input = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "# model = variable + placeholder\n",
    "y_pred = tf.add(tf.mul(x_input, weight_matrix), b_matrix)\n",
    "\n",
    "# loss function - for score\n",
    "loss = tf.reduce_mean(tf.square(y_actual - y_pred))\n",
    "\n",
    "\n",
    "# model initialize, train\n",
    "# 1.\n",
    "with tf.Session(graph=graph) as session:\n",
    "    ...\n",
    "    session.run(...)\n",
    "    ...\n",
    "# 2.\n",
    "session = tf.Session(graph=graph)\n",
    "session.run(...)\n",
    "\n",
    "\n",
    "# refer\n",
    "# https://www.tensorflow.org/api_docs/python/\n",
    "\n",
    "\n",
    "\n",
    "# tensor\n",
    "# 1. with zeros\n",
    "zero_tsr = tf.zeros([row_dim, col_dim])\n",
    "\n",
    "# 2. with ones\n",
    "ones_tsr = tf.ones([row_dim, col_dim])\n",
    "\n",
    "# 3. constant\n",
    "filled_tsr = tf.fill([row_dim, col_dim], 42)\n",
    "# [ = tf.constant(42, [row_dim, col_dim]) ]\n",
    "\n",
    "# 4. constant_2\n",
    "constant_tsr = tf.constant([1, 2, 3])\n",
    "\n",
    "# 5. same shape\n",
    "zeros_similar = tf.zeros_like(constant_tsr)\n",
    "ones_similar = tf.ones_like(constant_tsr)\n",
    "\n",
    "# 6. seq tensor : like range()\n",
    "linear_tsr = tf.linspace(start=0, stop=1, num=3)\n",
    "# => [ 0.0, 0.5, 1.0 ]\n",
    "\n",
    "integer_seq_tsr = tf.range(start=6, limit=15, delta=3)\n",
    "# => [ 6, 9, 12 ]\n",
    "\n",
    "# 7. random tensor\n",
    "randunif_tsr = tf.random_uniform([row_dim, col_dim], minval=0, maxval=1)\n",
    "# [ minval <= x < maxval ]\n",
    "\n",
    "# normal distribution\n",
    "randnorm_tsr = tf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0)\n",
    "\n",
    "# normal distribution + specific range\n",
    "# always pick -stddev <= x <= stddev\n",
    "runcnorm_tsr = tf.truncated_normal([row_dim, col_dim], mean=0.0, stddev=1.0)\n",
    "\n",
    "# shuffle array\n",
    "shuffled_output = tf.random_shuffle(input_tensor)\n",
    "cropped_output = tf.random_crop(input_tensor, crop_size)\n",
    "ex) --> cropped_image = tf.random_crop(my_image, [height/2, width/2, 3])\n",
    "\n",
    "\n",
    "# 8. variable\n",
    "my_var = tf.Variable(tf.zeros[row_dim, col_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file : [ res/contents/data.0 ]\n",
      "start read ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [03:11, 521.82it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file : [ res/contents/data.1 ]\n",
      "start read ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [03:04, 542.56it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file : [ res/contents/data.2 ]\n",
      "start read ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [03:19, 502.29it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file : [ res/contents/data.3 ]\n",
      "start read ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [03:21, 495.32it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file : [ res/contents/data.4 ]\n",
      "start read ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [03:19, 502.14it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file : [ res/contents/data.5 ]\n",
      "start read ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [03:20, 497.74it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file : [ res/contents/data.6 ]\n",
      "start read ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42190it [01:21, 518.68it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[ - word_dictionary ]\n",
    "[ - word_dictionary_rev ]\n",
    "\n",
    "* metadata 에 있는 정보만 유효하다.\n",
    "\n",
    "'''\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# get word dictionary \n",
    "# type 1 : { word : count }\n",
    "# type 2 : { word : idx }\n",
    "# type 3 : { idx : word }\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os \n",
    "TEMP_FOLDER = 'tmp/'\n",
    "RES_FOLDER = 'res/'\n",
    "from smart_open import open\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "\n",
    "meta_data_file = RES_FOLDER + 'metadata.json'\n",
    "\n",
    "word_dict_file = TEMP_FOLDER + 'tensor/word_dict'\n",
    "\n",
    "# word는 training data로부터 수집한다.\n",
    "# training data에 나오지 않은 word는 무시\n",
    "\n",
    "# word, count\n",
    "word_dict = {}\n",
    "        \n",
    "\n",
    "#file open_learning data file \n",
    "contentsSize = 7\n",
    "contents = [ os.path.join(RES_FOLDER, 'contents/data.' + str(i)) for i in range(contentsSize) ]\n",
    "# contentsLineSize = [ 100000, 100000, 100000, 100000, 100000, 100000, 42190 ]\n",
    "\n",
    "for i in range(contentsSize) :    \n",
    "    print('open file : [ ' + contents[i] + ' ]')\n",
    "    with open(contents[i], 'r') as f :\n",
    "        print('start read ...')\n",
    "        for jsonLine in tqdm.tqdm(f, mininterval=1) :\n",
    "            \n",
    "            line = json.JSONDecoder().decode(jsonLine)\n",
    "            \n",
    "            # collect only word\n",
    "            \n",
    "            # doc_id\n",
    "            # doc_id = line['id']\n",
    "            \n",
    "            # morphs\n",
    "            for morphs in line['morphs'] :\n",
    "                for mor in morphs :\n",
    "                    word_dict[mor] = word_dict.get(mor, 0) + 1\n",
    "        \n",
    "        \n",
    "            \n",
    "with open(word_dict_file, 'w') as f :\n",
    "    f.write(json.JSONEncoder().encode(word_dict))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "idx 3: 4/VV\n",
      "4/VV: 3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- make \n",
    "[ word2idx ], [ idx2word ]\n",
    "'''\n",
    "\n",
    "\n",
    "# word to idx\n",
    "word2idx = {}\n",
    "\n",
    "# idx to word\n",
    "idx2word = {}\n",
    "\n",
    "for word in word_dict :\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "\n",
    "with open(TEMP_FOLDER+'tensor/word2idx', 'w') as f :\n",
    "    f.write(json.JSONEncoder().encode(word2idx))\n",
    "    \n",
    "with open(TEMP_FOLDER+'tensor/idx2word', 'w') as f :\n",
    "    f.write(json.JSONEncoder().encode(idx2word))\n",
    "\n",
    "\n",
    "print('check')\n",
    "print('idx 3: '+idx2word[3])\n",
    "print(idx2word[3]+': ' + str(word2idx[idx2word[3]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-af07df495673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "TEMP_FOLDER = 'tmp/'\n",
    "\n",
    "# load word_dict, word2idx, idx2word\n",
    "\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "with open(TEMP_FOLDER+'tensor/word2idx', 'r') as f :\n",
    "    word2idx = json.JSONDecoder().decode(f.read())\n",
    "    \n",
    "with open(TEMP_FOLDER+'tensor/idx2word', 'r') as f :\n",
    "    idx2word = json.JSONDecoder().decode(f.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(word2idx['1/NNG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(meta_data_file, 'r') as f :\n",
    "    for raw in f :\n",
    "        line = json.JSONDecoder().decode(raw)\n",
    "        # line\n",
    "        '''\n",
    "        # {'magazine_id': 8982, \n",
    "            'user_id': '@bookdb', \n",
    "            'title': '사진으로 옮기기에도 아까운, 리치필드 국립공원', \n",
    "            'keyword_list': ['여행', '호주', '국립공원'], \n",
    "            'display_url': 'https://brunch.co.kr/@bookdb/782', \n",
    "            'sub_title': '세상 어디에도 없는 호주 Top 10', \n",
    "            'reg_ts': 1474944427000, \n",
    "            'article_id': 782, \n",
    "            'id': '@bookdb_782'}\n",
    "        '''\n",
    "        \n",
    "        for word in line['']\n",
    "        \n",
    "        print(line)\n",
    "        break\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
