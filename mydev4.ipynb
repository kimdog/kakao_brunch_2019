{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"tmp/\" will be used to save temporary dictionary and corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-02 10:27:59,688 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import logging \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) \n",
    "import os \n",
    "import tempfile \n",
    "TEMP_FOLDER = 'tmp/'\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER)) \n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim') \n",
    "from gensim import corpora \n",
    "import gensim \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import csv \n",
    "import collections \n",
    "from smart_open import open\n",
    "import random \n",
    "from pprint import pprint  # pretty-printer \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1/NNG': 0}\n"
     ]
    }
   ],
   "source": [
    "# wordDic : word Dictionary\n",
    "with open(TEMP_FOLDER+'wordDic_1', 'r') as wdf :\n",
    "    wordDic = json.JSONDecoder().decode(wdf.read())\n",
    "    \n",
    "\n",
    "# word2idx\n",
    "word2idx = {}\n",
    "with open(TEMP_FOLDER+'word2idx_1', 'w') as w2i :\n",
    "    for i, k in enumerate(wordDic) :\n",
    "        word2idx[k] = i\n",
    "    \n",
    "    w2i.write(json.JSONEncoder().encode(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(TEMP_FOLDER+'word2idx_1', 'r') as w2i :\n",
    "    word2idx = json.JSONDecoder().decode(w2i.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@bookdb_782\n",
      "@bookfit_2518\n",
      "@ladycc_44\n",
      "@mvpreborn_7\n",
      "@c4u_431\n",
      "@brunchnwkd_133\n",
      "@memojang_89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SIZE_OF_CHUNK = 5000\n",
    "\n",
    "\n",
    "#function definition \n",
    "def read_corpus(f): \n",
    "    count = 0;\n",
    "    for jsonLine in f :\n",
    "        \n",
    "        line = json.JSONDecoder().decode(jsonLine)\n",
    "        doc_id = line['id']\n",
    "        \n",
    "        if count == 0 : print(doc_id)\n",
    "        \n",
    "        for morphs in line['morphs'] :\n",
    "            # store word as index\n",
    "            morphList = [ word2idx[morph] for morph in morphs ]\n",
    "            yield gensim.models.doc2vec.TaggedDocument(morphList, [doc_id])\n",
    "            \n",
    "        count = count + 1\n",
    "        if count >= SIZE_OF_CHUNK : break\n",
    "        \n",
    "            \n",
    "#file open_learning data file \n",
    "resFolder = 'res/'\n",
    "contentsSize = 7\n",
    "contents = [ open(os.path.join(resFolder, 'contents/data.' + str(i)), 'r') for i in range(contentsSize) ]\n",
    "# contents = [ os.path.join(resFolder, 'contents/data.' + str(i)) for i in range(contentsSize) ]\n",
    "contentsLineSize = [ 100000, 100000, 100000, 100000, 100000, 100000, 42190 ]\n",
    "\n",
    "train_corpus = list()\n",
    "for i in range(contentsSize) :    \n",
    "    for j in range( int(( contentsLineSize[i] + SIZE_OF_CHUNK - 1 ) / SIZE_OF_CHUNK ) ) :\n",
    "        # read corpus\n",
    "        train_corpus.extend( list(read_corpus(contents[i])) ) \n",
    "\n",
    "    contents[i].close()\n",
    "\n",
    "\n",
    "#build a model \n",
    "model = gensim.models.doc2vec.Doc2Vec(\n",
    "    alpha=0.025,\n",
    "    min_alpha=0.025,\n",
    "    window=5,\n",
    "    size=20, \n",
    "    min_count=5, \n",
    "    iter=1\n",
    ")\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        \n",
    "#save a model and word, sentence vector\n",
    "model.save( TEMP_FOLDER+'model_test_1.model') \n",
    "model.save_word2vec_format(TEMP_FOLDER+'model_test_1_format', doctag_vec=True, word_vec=True, prefix='*dt_', fvocab=None, binary=False) \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.doc2vec.Doc2Vec'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the model \n",
    "model = gensim.models.Doc2Vec.load( TEMP_FOLDER+'model_test_1.model' )  # you can continue training with the loaded model! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
